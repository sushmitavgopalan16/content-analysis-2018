{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 : Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For NLP\n",
    "import nltk\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas as pd #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "import lucem_illud.stanford as stanford\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Examine the list of words associated with at least three different parts of speech. Consider conditional frequencies (e.g., adjectives associated with nouns of interest or adverbs with verbs of interest). What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"week7.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "redditTopScores = df\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.1 ms, sys: 194 ms, total: 259 ms\n",
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "redditTopScores['POS_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[(``, ``), (I, PRP), (was, VBD), (delighted, ...\n",
       "1    [[(During, IN), (their, PRP$), (first, JJ), (1...\n",
       "2    [[(Last, JJ), (week, NN), (,, ,), (Ron, NNP), ...\n",
       "3    [[(``, ``), (I, PRP), (certainly, RB), (do, VB...\n",
       "4    [[(Musical-theater, NN), (productions, NNS), (...\n",
       "5    [[(During, IN), (the, DT), (past, JJ), (few, J...\n",
       "6    [[(For, IN), (years, NNS), (,, ,), (Def, NNP),...\n",
       "7    [[(On, IN), (February, NNP), (12th, JJ), (,, ,...\n",
       "8    [[(``, ``), (I, PRP), ('ve, VBP), (never, RB),...\n",
       "9    [[(As, IN), (it, PRP), (turns, VBZ), (out, RP)...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['POS_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('band', 55),\n",
       " ('album', 38),\n",
       " ('\\x89ÛÒ', 35),\n",
       " ('music', 35),\n",
       " ('time', 26),\n",
       " ('year', 23),\n",
       " ('nbsp', 23),\n",
       " (')', 22),\n",
       " ('song', 22),\n",
       " ('show', 21),\n",
       " ('thing', 18),\n",
       " ('rock', 17),\n",
       " ('lot', 17),\n",
       " ('amp', 17),\n",
       " ('way', 17),\n",
       " ('something', 15),\n",
       " ('everything', 14),\n",
       " ('record', 14),\n",
       " (']', 13),\n",
       " ('tour', 12)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 48),\n",
       " ('do', 34),\n",
       " ('have', 27),\n",
       " ('go', 20),\n",
       " ('make', 16),\n",
       " ('say', 15),\n",
       " ('get', 15),\n",
       " ('know', 15),\n",
       " ('play', 13),\n",
       " ('sing', 9),\n",
       " ('come', 8),\n",
       " ('see', 8),\n",
       " ('want', 8),\n",
       " ('think', 6),\n",
       " ('take', 6),\n",
       " ('keep', 6),\n",
       " ('give', 6),\n",
       " ('work', 5),\n",
       " ('look', 4),\n",
       " ('try', 4)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'own', 'Dead', 'good', 'commercial', 'subscription-based', 'contemporary', 'rough'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'music'\n",
    "NResults = set()\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treeBank = nltk.corpus.treebank\n",
    "treeBank.tagged_sents()[0]\n",
    "treeBank.sents()[0]\n",
    "stanfordTags = stanford.postTagger.tag_sents(treeBank.sents()[:30])\n",
    "NumDiffs = 0\n",
    "for sentIndex in range(len(stanfordTags)):\n",
    "    for wordIndex in range(len(stanfordTags[sentIndex])):\n",
    "        if stanfordTags[sentIndex][wordIndex][1] != treeBank.tagged_sents()[sentIndex][wordIndex][1]:\n",
    "            if treeBank.tagged_sents()[sentIndex][wordIndex][1] != '-NONE-':\n",
    "                print(\"Word: {}  \\tStanford: {}\\tTreebank: {}\".format(stanfordTags[sentIndex][wordIndex][0], stanfordTags[sentIndex][wordIndex][1], treeBank.tagged_sents()[sentIndex][wordIndex][1]))\n",
    "                NumDiffs += 1\n",
    "total = sum([len(s) for s in stanfordTags])\n",
    "print(\"The Precision is {:.3f}%\".format((total-NumDiffs)/total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of your corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for the NER classification just performed (using your own hand-codings as \"ground truth\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['classified_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[(``, O), (I, O), (was, O), (delighted, O), (...\n",
       "1    [[(During, O), (their, O), (first, O), (10, O)...\n",
       "2    [[(Last, O), (week, O), (,, O), (Ron, PERSON),...\n",
       "3    [[(``, O), (I, O), (certainly, O), (do, O), (n...\n",
       "4    [[(Musical-theater, O), (productions, O), (ten...\n",
       "5    [[(During, O), (the, O), (past, O), (few, O), ...\n",
       "6    [[(For, O), (years, O), (,, O), (Def, PERSON),...\n",
       "7    [[(On, O), (February, O), (12th, O), (,, O), (...\n",
       "8    [[(``, O), (I, O), ('ve, O), (never, O), (felt...\n",
       "9    [[(As, O), (it, O), (turns, O), (out, O), (,, ...\n",
       "Name: classified_sents, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['classified_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 908),\n",
       " ('.', 714),\n",
       " ('the', 619),\n",
       " ('to', 376),\n",
       " ('and', 340),\n",
       " ('a', 330),\n",
       " ('``', 323),\n",
       " ('of', 274),\n",
       " ('in', 251),\n",
       " (\"''\", 229)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['streak',\n",
       " 'Hook',\n",
       " 'busy',\n",
       " 'Ron',\n",
       " 'mistakes',\n",
       " 'BeyoncÌ©',\n",
       " 'added',\n",
       " 'strings',\n",
       " 'rarely',\n",
       " 'earliest',\n",
       " 'upset',\n",
       " 'loop',\n",
       " 'landed',\n",
       " 'suddenly',\n",
       " 'meetings',\n",
       " 'Good',\n",
       " 'Shakedown',\n",
       " 'hold',\n",
       " 'concussions',\n",
       " 'Into',\n",
       " 'associated',\n",
       " 'everyone',\n",
       " 'nomination',\n",
       " 'light',\n",
       " 'lots',\n",
       " 'reflected',\n",
       " 'terms',\n",
       " 'crack',\n",
       " 'nowhere',\n",
       " 'current',\n",
       " 'stop',\n",
       " 'Robert',\n",
       " 'While',\n",
       " 'produced',\n",
       " 'signing',\n",
       " 'Keystone',\n",
       " 'taken',\n",
       " 'accompanied',\n",
       " 'characters',\n",
       " 'nice',\n",
       " 'buy',\n",
       " 'dressing',\n",
       " 'core',\n",
       " 'offers',\n",
       " 'sign',\n",
       " 'Hong',\n",
       " 'response',\n",
       " 'Beers',\n",
       " 'Youngs',\n",
       " 'possible',\n",
       " 'stream',\n",
       " 'president',\n",
       " 'publishing',\n",
       " 'instrument',\n",
       " 'agreed',\n",
       " 'tunes',\n",
       " 'mother',\n",
       " 'air',\n",
       " 'job',\n",
       " 'starts',\n",
       " 'working',\n",
       " 'Night',\n",
       " 'Kong',\n",
       " 'drive',\n",
       " 'former',\n",
       " 'happy',\n",
       " 'understood',\n",
       " 'dollar',\n",
       " 'press',\n",
       " '1978',\n",
       " 'Jay-Z',\n",
       " 'football',\n",
       " 'door',\n",
       " 'numbers',\n",
       " 'energy',\n",
       " 'Seven',\n",
       " 'love',\n",
       " 'sings',\n",
       " 'Now',\n",
       " 'familiar',\n",
       " 'choose',\n",
       " 'foresight',\n",
       " 'Words',\n",
       " 'wonderful',\n",
       " 'led',\n",
       " 'bassist',\n",
       " 'heart',\n",
       " 'categories',\n",
       " 'Live',\n",
       " 'honor',\n",
       " 'skepticism',\n",
       " 'mind',\n",
       " 'draw',\n",
       " 'breaking',\n",
       " 'line',\n",
       " 'lyrics',\n",
       " 'thanks',\n",
       " 'Felder',\n",
       " '$',\n",
       " 'son',\n",
       " 'knowledge',\n",
       " 'festival',\n",
       " 'inside',\n",
       " 'party',\n",
       " 'At',\n",
       " 'period',\n",
       " 'efforts',\n",
       " 'High',\n",
       " 'formed',\n",
       " 'Four',\n",
       " 'factor',\n",
       " 'based',\n",
       " 'admits',\n",
       " 'myself',\n",
       " 'vaults',\n",
       " 'funny',\n",
       " 'decided',\n",
       " 'learn',\n",
       " 'Off-Broadway',\n",
       " 'backstage',\n",
       " '2008',\n",
       " 'experience',\n",
       " '15',\n",
       " 'following',\n",
       " 'collaborate',\n",
       " 'vampire',\n",
       " 'Ole',\n",
       " 'currently',\n",
       " 'bed',\n",
       " 'general',\n",
       " 'EP',\n",
       " 'ground',\n",
       " 'echoed',\n",
       " 'big-money',\n",
       " '2009',\n",
       " 'kid',\n",
       " 'eligible',\n",
       " 'target',\n",
       " 'Gramm',\n",
       " 'minute',\n",
       " 'debuts',\n",
       " 'terrible',\n",
       " 'effort',\n",
       " 'shame',\n",
       " 'Bad',\n",
       " 'company',\n",
       " 'Liberty',\n",
       " 'sang',\n",
       " 'versions',\n",
       " 'massive',\n",
       " '1974',\n",
       " 'hand',\n",
       " 'announcement',\n",
       " 'landmark',\n",
       " 'difficult',\n",
       " 'large',\n",
       " '1979',\n",
       " 'grew',\n",
       " 'Marcus',\n",
       " 'Shreveport',\n",
       " 'diverse',\n",
       " 'received',\n",
       " 'Blues',\n",
       " 'business',\n",
       " 'creating',\n",
       " 'against',\n",
       " 'recordings',\n",
       " 'ways',\n",
       " 'performing',\n",
       " 'Hey',\n",
       " 'tried',\n",
       " '1997',\n",
       " 'Those',\n",
       " 'America',\n",
       " 'fun',\n",
       " \"'This\",\n",
       " 'Facebook',\n",
       " 'Jim',\n",
       " 'change',\n",
       " 'January',\n",
       " 'room',\n",
       " 'small',\n",
       " 'director',\n",
       " 'Despacito',\n",
       " 'D.R.A.M',\n",
       " 'arenas',\n",
       " 'II',\n",
       " 'powerful',\n",
       " 'warble',\n",
       " 'fame',\n",
       " 'Universal',\n",
       " 'vibe',\n",
       " 'Paradise',\n",
       " 'dementia',\n",
       " 'Born',\n",
       " 'rehearsal',\n",
       " 'York',\n",
       " 'Loved',\n",
       " 'Grand',\n",
       " 'Which',\n",
       " 'walk',\n",
       " 'Apple',\n",
       " 'spent',\n",
       " 'leading',\n",
       " 'reasons',\n",
       " 'Of',\n",
       " 'Gambino',\n",
       " 'foundation',\n",
       " 'Blue',\n",
       " 'closed',\n",
       " 'service',\n",
       " 'octave',\n",
       " 'claims',\n",
       " 'instruments',\n",
       " 'imagine',\n",
       " 'pretend',\n",
       " 'vocal',\n",
       " 'meeting',\n",
       " 'plays',\n",
       " '18',\n",
       " 'native',\n",
       " 'Seventies',\n",
       " 'broadcast',\n",
       " 'similar',\n",
       " 'becoming',\n",
       " 'Las',\n",
       " 'image',\n",
       " \"'No\",\n",
       " 'fast',\n",
       " 'songwriting',\n",
       " 'looking',\n",
       " 'sadness',\n",
       " 'submit',\n",
       " 'sell',\n",
       " 'Walsh',\n",
       " 'pissed',\n",
       " 'According',\n",
       " 'LP',\n",
       " '--',\n",
       " 'Butterfly',\n",
       " 'permission',\n",
       " 'plot',\n",
       " 'eye',\n",
       " 'presence',\n",
       " 'question',\n",
       " 'rockers',\n",
       " 'These',\n",
       " 'teamed',\n",
       " 'blues',\n",
       " 'diversity',\n",
       " 'sounded',\n",
       " 'weight',\n",
       " 'Box',\n",
       " 'Mainstream',\n",
       " 'Stadium',\n",
       " 'stops',\n",
       " 'living',\n",
       " 'Roy',\n",
       " 'sensed',\n",
       " 'concert',\n",
       " 'god',\n",
       " 'earlier',\n",
       " 'onto',\n",
       " 'Entertainment',\n",
       " 'Diamond',\n",
       " 'piano',\n",
       " 'laid',\n",
       " 'interpretation',\n",
       " 'towards',\n",
       " 'big-ass',\n",
       " 'toured',\n",
       " 'bottle',\n",
       " 'force',\n",
       " 'Rose',\n",
       " 'promotions',\n",
       " 'Foley',\n",
       " 'Jones',\n",
       " 'health',\n",
       " 'Show',\n",
       " 'gonzo',\n",
       " 'contract',\n",
       " 'peaked',\n",
       " 'walked',\n",
       " 'credits',\n",
       " 'microphone',\n",
       " 'magazine',\n",
       " 'milestone',\n",
       " 'paints',\n",
       " 'bats',\n",
       " 'passed',\n",
       " \"'\\x89Ûä\",\n",
       " 'ballad',\n",
       " 'arrangements',\n",
       " 'rough',\n",
       " 'school',\n",
       " 'July',\n",
       " 'speaks',\n",
       " 'connect',\n",
       " 'eventually',\n",
       " 'verse',\n",
       " 'nails',\n",
       " 'Since',\n",
       " 'Bruno',\n",
       " 'drugs',\n",
       " 'keyboardist',\n",
       " '200',\n",
       " 'sort',\n",
       " 'taste',\n",
       " 'bands',\n",
       " 'fair',\n",
       " 'However',\n",
       " 'Australian',\n",
       " 'beloved',\n",
       " 'singing',\n",
       " 'Migos',\n",
       " 'discovered',\n",
       " 'opportunities',\n",
       " 'Look',\n",
       " '1973',\n",
       " 'dark',\n",
       " 'completed',\n",
       " 'Nobody',\n",
       " 'sessions',\n",
       " 'weekend',\n",
       " 'records',\n",
       " 'scene',\n",
       " 'worldwide',\n",
       " 'vocals',\n",
       " '1991',\n",
       " 'hits',\n",
       " 'clip',\n",
       " 'fuck',\n",
       " 'ballads',\n",
       " 'advice',\n",
       " 'actor',\n",
       " 'choir',\n",
       " 'broke',\n",
       " 'Def',\n",
       " 'fit',\n",
       " 'Forum',\n",
       " 'contemporary',\n",
       " 'Love',\n",
       " 'Johnny',\n",
       " 'confidence',\n",
       " '1988',\n",
       " 'yeah',\n",
       " '40th',\n",
       " 'share',\n",
       " 'Everyone',\n",
       " 'must',\n",
       " 'presented',\n",
       " 'tells',\n",
       " 'largely',\n",
       " 'initially',\n",
       " 'popular',\n",
       " 'grade',\n",
       " 'named',\n",
       " 'addition',\n",
       " 'quite',\n",
       " 'totally',\n",
       " 'respectively',\n",
       " 'pool',\n",
       " 'platinum',\n",
       " 'stupid',\n",
       " 'musicians',\n",
       " 'February',\n",
       " 'England',\n",
       " 'written',\n",
       " 'Gary',\n",
       " 'performances',\n",
       " 'auditioning',\n",
       " 'Roll',\n",
       " 'miserable',\n",
       " 'knife',\n",
       " 'co-founder',\n",
       " 'blend',\n",
       " 'jam',\n",
       " 'multiple',\n",
       " 'completely',\n",
       " 'Deadheads',\n",
       " 'Pimp',\n",
       " \"'em\",\n",
       " '6lack',\n",
       " 'playlists',\n",
       " 'player',\n",
       " 'cutting',\n",
       " 'however',\n",
       " 'Ultimately',\n",
       " 'older',\n",
       " 'member',\n",
       " 'Frey',\n",
       " '1966',\n",
       " 'dead',\n",
       " 'remembers',\n",
       " 'harmony',\n",
       " 'Bittan',\n",
       " 'rising',\n",
       " 'Scott',\n",
       " 'blogs',\n",
       " 'road',\n",
       " 'touch',\n",
       " 'quality',\n",
       " 'kicked',\n",
       " 'turnout',\n",
       " 'short',\n",
       " 'no-brainer',\n",
       " 'refused',\n",
       " 'Statue',\n",
       " 'agency',\n",
       " 'happening',\n",
       " 'coming',\n",
       " 'Cash',\n",
       " '12-pound',\n",
       " 'sent',\n",
       " 'Their',\n",
       " 'ended',\n",
       " 'schedule',\n",
       " 'nearly',\n",
       " 'film',\n",
       " 'minutes',\n",
       " 'watched',\n",
       " 'partnership',\n",
       " 'drawn',\n",
       " 'anyone',\n",
       " 'Leppard',\n",
       " 'speak',\n",
       " 'co-producer',\n",
       " 'information',\n",
       " 'creative',\n",
       " 'Grateful',\n",
       " 'sword',\n",
       " 'releases',\n",
       " 'Mary',\n",
       " 'Morris',\n",
       " 'drew',\n",
       " 'writes',\n",
       " 'beat',\n",
       " 'caught',\n",
       " 'Legion',\n",
       " 'demo',\n",
       " 'further',\n",
       " 'Before',\n",
       " 'November',\n",
       " 'background',\n",
       " 'community',\n",
       " 'including',\n",
       " '2005',\n",
       " 'consideration',\n",
       " 'human']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Garcia', 20),\n",
       " ('Malcolm', 14),\n",
       " ('Hutchence', 13),\n",
       " ('INXS', 12),\n",
       " ('Andrew', 11),\n",
       " ('Murphy', 11),\n",
       " ('Mann', 11),\n",
       " ('Bennett', 11),\n",
       " ('Steadman', 11),\n",
       " ('Jerry', 11)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('INXS', 12),\n",
       " ('Academy', 8),\n",
       " ('Rolling', 8),\n",
       " ('Stone', 7),\n",
       " ('Recording', 7),\n",
       " ('Grammys', 4),\n",
       " ('Gold', 4),\n",
       " ('Green', 4),\n",
       " ('Ocean', 3),\n",
       " ('Marcus', 2)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceived to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
